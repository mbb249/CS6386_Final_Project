{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections.abc import Mapping, Container\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage(bf):\n",
    "    \"\"\"Estimate memory used by each filter.\"\"\"\n",
    "    size = sys.getsizeof(bf)\n",
    "\n",
    "    # classical bit/count arrays\n",
    "    if hasattr(bf, 'bit_array'):\n",
    "        size += sys.getsizeof(bf.bit_array)\n",
    "    if hasattr(bf, 'count_array'):\n",
    "        size += sys.getsizeof(bf.count_array)\n",
    "\n",
    "    # Sandwich: just sum its two parts\n",
    "    if hasattr(bf, 'small') and hasattr(bf, 'ml'):\n",
    "        return get_memory_usage(bf.ml) + get_memory_usage(bf.small)\n",
    "\n",
    "    # any ML‐based filter with a .model\n",
    "    if hasattr(bf, 'model'):\n",
    "        mdl = bf.model\n",
    "\n",
    "        # neural networks & logistic regression have coefs_ / intercepts_\n",
    "        if hasattr(mdl, 'coefs_'):\n",
    "            for coef in mdl.coefs_:\n",
    "                size += coef.nbytes\n",
    "            for intercept in mdl.intercepts_:\n",
    "                size += intercept.nbytes\n",
    "\n",
    "        # random forests have many tree estimators\n",
    "        if hasattr(mdl, 'estimators_'):\n",
    "            for tree_est in mdl.estimators_:\n",
    "                tree = tree_est.tree_\n",
    "                # pick the big arrays inside each tree\n",
    "                for arr_name in ('threshold', 'feature', 'children_left',\n",
    "                                 'children_right', 'value'):\n",
    "                    arr = getattr(tree, arr_name, None)\n",
    "                    if isinstance(arr, np.ndarray):\n",
    "                        size += arr.nbytes\n",
    "\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardBloomFilter:\n",
    "    def __init__(self, n, fp_rate):\n",
    "        self.size = self._get_size(n, fp_rate)\n",
    "        self.hash_count = self._get_hash_count(self.size, n)\n",
    "        self.bit_array = [0] * self.size\n",
    "\n",
    "    def _hashes(self, item):\n",
    "        return [hashlib.sha256(f\"{item}{i}\".encode()).hexdigest() for i in range(self.hash_count)]\n",
    "\n",
    "    def _get_size(self, n, p):\n",
    "        m = -(n * math.log(p)) / (math.log(2)**2)\n",
    "        return int(m)\n",
    "\n",
    "    def _get_hash_count(self, m, n):\n",
    "        return int((m / n) * math.log(2))\n",
    "\n",
    "    def add(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            idx = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(), 16) % self.size\n",
    "            self.bit_array[idx] = 1\n",
    "\n",
    "    def query(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            idx = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(), 16) % self.size\n",
    "            if self.bit_array[idx] == 0:\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountingBloomFilter:\n",
    "    def __init__(self, n, fp_rate):\n",
    "        self.size = self._get_size(n, fp_rate)\n",
    "        self.hash_count = self._get_hash_count(self.size, n)\n",
    "        self.count_array = [0] * self.size\n",
    "\n",
    "    def _get_size(self, n, p):\n",
    "        return int(-(n * math.log(p)) / (math.log(2)**2))\n",
    "\n",
    "    def _get_hash_count(self, m, n):\n",
    "        return int((m / n) * math.log(2))\n",
    "\n",
    "    def add(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            idx = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(), 16) % self.size\n",
    "            self.count_array[idx] += 1\n",
    "\n",
    "    def remove(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            idx = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(), 16) % self.size\n",
    "            self.count_array[idx] = max(0, self.count_array[idx] - 1)\n",
    "\n",
    "    def query(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            idx = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(), 16) % self.size\n",
    "            if self.count_array[idx] == 0:\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMLBloomFilter:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.trained = False\n",
    "\n",
    "    def _featurize(self, item):\n",
    "        arr = [ord(c) / 255.0 for c in item[:50]] + [0.0]*(50 - len(item))\n",
    "        length = len(item) / 50.0\n",
    "        hash_val = int(hashlib.md5(item.encode()).hexdigest(), 16) % 1000000 / 1000000.0\n",
    "        return np.array(arr + [length, hash_val]).reshape(1, -1)\n",
    "\n",
    "    def train(self, positives, negatives):\n",
    "        X = np.vstack([self._featurize(x) for x in positives + negatives])\n",
    "        y = np.array([1]*len(positives) + [0]*len(negatives))\n",
    "        self.model.fit(X, y)\n",
    "        self.trained = True\n",
    "\n",
    "    def query(self, item):\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Model must be trained before querying.\")\n",
    "        x = self._featurize(item)\n",
    "        return bool(self.model.predict(x)[0])\n",
    "\n",
    "    def add(self, item):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkBloomFilter(BaseMLBloomFilter):\n",
    "    def __init__(self):\n",
    "        model = MLPClassifier(hidden_layer_sizes=(20,), max_iter=300, alpha=0.001)\n",
    "        super().__init__(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Bloom Filter\n",
    "class LogisticRegressionBloomFilter(BaseMLBloomFilter):\n",
    "    def __init__(self):\n",
    "        model = LogisticRegression(max_iter=200)\n",
    "        super().__init__(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestBloomFilter(BaseMLBloomFilter):\n",
    "    def __init__(self, max_model_size_bytes=1500000, n_estimators=100, max_depth=10):\n",
    "        self.size_limit = max_model_size_bytes\n",
    "        self.base_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "        super().__init__(self.base_model)\n",
    "\n",
    "    def train(self, positives, negatives):\n",
    "        X = np.vstack([self._featurize(x) for x in positives + negatives])\n",
    "        y = np.array([1]*len(positives) + [0]*len(negatives))\n",
    "        \n",
    "        # Try reducing depth to meet memory limit\n",
    "        for depth in range(self.base_model.max_depth, 0, -1):\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=self.base_model.n_estimators,\n",
    "                max_depth=depth\n",
    "            )\n",
    "            model.fit(X, y)\n",
    "            self.model = model\n",
    "            size = get_memory_usage(self)\n",
    "            if size <= self.size_limit:\n",
    "                print(f\"✅ depth={depth} fits: {size:,} bytes ≤ {self.size_limit:,}\")\n",
    "                self.trained = True\n",
    "                return\n",
    "            else:\n",
    "                print(f\"⚠️  depth={depth} exceeds: {size:,} bytes > {self.size_limit:,}\")\n",
    "\n",
    "        # fallback\n",
    "        print(\"❌ Could not meet memory constraint, using depth=1\")\n",
    "        self.model = RandomForestClassifier(n_estimators=self.base_model.n_estimators, max_depth=1)\n",
    "        self.model.fit(X, y)\n",
    "        self.trained = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMBloomFilter(BaseMLBloomFilter):\n",
    "    def __init__(self):\n",
    "        model = SVC(probability=True, kernel='linear', max_iter=1000)\n",
    "        super().__init__(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "class SGDLogisticBloomFilter(BaseMLBloomFilter):\n",
    "    def __init__(self):\n",
    "        model = SGDClassifier(loss='log_loss', max_iter=1000)\n",
    "        super().__init__(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LightGBMBloomFilter(BaseMLBloomFilter):\n",
    "    def __init__(self, max_model_size_bytes=1500000):\n",
    "        self.size_limit = max_model_size_bytes\n",
    "        model = lgb.LGBMClassifier(max_depth=7, n_estimators=100)\n",
    "        super().__init__(model)\n",
    "\n",
    "    def train(self, positives, negatives):\n",
    "        X = np.vstack([self._featurize(x) for x in positives + negatives])\n",
    "        y = np.array([1]*len(positives) + [0]*len(negatives))\n",
    "        self.model.fit(X, y)\n",
    "        size = get_memory_usage(self)\n",
    "        if size > self.size_limit:\n",
    "            print(f\"⚠️ LightGBM model too large: {size:,} bytes\")\n",
    "        else:\n",
    "            print(f\"✅ LightGBM model fits: {size:,} bytes\")\n",
    "        self.trained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SandwichBloomFilter:\n",
    "    def __init__(self, ml_filter, positives, negatives, fp_rate_small=0.20):\n",
    "        self.ml = ml_filter\n",
    "        self.ml.train(positives, negatives)\n",
    "        self.small = StandardBloomFilter(len(positives), fp_rate_small)\n",
    "        self._initialize_small_filter(positives)\n",
    "\n",
    "    def _initialize_small_filter(self, positives):\n",
    "        for item in positives:\n",
    "            if not self.ml.query(item):  # Only ML misses\n",
    "                self.small.add(item)\n",
    "\n",
    "    def add(self, item):\n",
    "        pass\n",
    "\n",
    "    def query(self, item):\n",
    "        if self.ml.query(item):\n",
    "            return True\n",
    "        else:\n",
    "            return self.small.query(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_readable_size(num_bytes, decimals=2):\n",
    "    for unit in ('B','KB','MB','GB','TB'):\n",
    "        if num_bytes < 1024.0:\n",
    "            return f\"{num_bytes:.{decimals}f} {unit}\"\n",
    "        num_bytes /= 1024.0\n",
    "    return f\"{num_bytes:.{decimals}f} PB\"\n",
    "\n",
    "def create_set(size):\n",
    "    my_set = set()\n",
    "    for i in range(size):\n",
    "        my_set.add(i)\n",
    "    return my_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(bf, positives, negatives):\n",
    "    \"\"\"Run insertions and then measure FP, FN, F1, timing, throughput, memory.\"\"\"\n",
    "    # — insert positives —\n",
    "    for url in positives:\n",
    "        bf.add(url)\n",
    "\n",
    "    # — measure false positives —\n",
    "    start = time.time()\n",
    "    false_positives = sum(1 for url in negatives if bf.query(url) and url not in positives)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # — measure false negatives (only for ML filters) —\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    if isinstance(bf, BaseMLBloomFilter):\n",
    "        for url in positives:\n",
    "            if bf.query(url):\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_negatives += 1\n",
    "\n",
    "    fpr = false_positives / len(negatives)\n",
    "    fnr = false_negatives / len(positives) if positives else 0.0\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / len(positives) if positives else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    avg_query_time = elapsed / len(negatives)\n",
    "    throughput = len(negatives) / elapsed if elapsed > 0 else float('inf')\n",
    "    memory_bytes = get_memory_usage(bf)\n",
    "\n",
    "    print(f\"\\n=== {bf.__class__.__name__} ===\")\n",
    "    print(f\"Memory Usage:             {memory_bytes:,} bytes\")\n",
    "    print(f\"False‐Positive Rate:      {fpr:.4%}\")\n",
    "    if isinstance(bf, BaseMLBloomFilter):\n",
    "        print(f\"False‐Negative Rate:      {fnr:.4%}\")\n",
    "        print(f\"F1 Score:                 {f1_score:.4f}\")\n",
    "    print(f\"Avg Query Time:           {avg_query_time:.6f} s\")\n",
    "    print(f\"Throughput:               {throughput:,.0f} queries/s\")\n",
    "\n",
    "    return {\n",
    "    'FP Rate': fpr,\n",
    "    'FN Rate': fnr if isinstance(bf, BaseMLBloomFilter) else np.nan,\n",
    "    'F1': f1_score if isinstance(bf, BaseMLBloomFilter) else np.nan,\n",
    "    'Avg Time (s)': avg_query_time,\n",
    "    'Throughput (q/s)': throughput,\n",
    "    'Mem (bytes)': memory_bytes\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_name, pos_path, neg_path):\n",
    "    print(f\"\\n🔍 {dataset_name} Bloom Filter Evaluation\\n\")\n",
    "\n",
    "    with open(pos_path) as f:\n",
    "        positives = f.read().splitlines()\n",
    "    with open(neg_path) as f:\n",
    "        negatives = f.read().splitlines()\n",
    "\n",
    "    n = len(positives)\n",
    "    size = get_memory_usage(create_set(n))\n",
    "\n",
    "    # Define all filters\n",
    "    ml_filters = [\n",
    "        (\"NeuralNet\", NeuralNetworkBloomFilter()),\n",
    "        (\"LightGBM\", LightGBMBloomFilter(max_model_size_bytes=size)),\n",
    "        (\"SGDLogistic\", SGDLogisticBloomFilter()),\n",
    "        (\"SVM\", SVMBloomFilter()),\n",
    "         (\"RandomForest\", RandomForestBloomFilter(max_model_size_bytes=size, n_estimators=100, max_depth=7))\n",
    "    ]\n",
    "\n",
    "    classical_filters = [\n",
    "        (\"Standard\", StandardBloomFilter(n, 0.1)),\n",
    "        (\"Counting\", CountingBloomFilter(n, 0.1)),\n",
    "    ]\n",
    "\n",
    "    # Train ML filters\n",
    "    for _, ml in ml_filters:\n",
    "        ml.train(positives, negatives)\n",
    "\n",
    "    filters = classical_filters + ml_filters\n",
    "\n",
    "    # Add Sandwich filters\n",
    "    filters += [\n",
    "        (f\"Sandwich-{name}\", SandwichBloomFilter(bf, positives, negatives, 0.2))\n",
    "        for name, bf in ml_filters\n",
    "    ]\n",
    "\n",
    "    # Evaluate all\n",
    "    results = []\n",
    "    for name, bf in filters:\n",
    "        m = evaluate(bf, positives, negatives)\n",
    "        results.append({\n",
    "            \"Filter\":            name,\n",
    "            \"Mem (bytes)\":       m[\"Mem (bytes)\"],\n",
    "            \"FP Rate\":           m[\"FP Rate\"],\n",
    "            \"FN Rate\":           m[\"FN Rate\"],\n",
    "            \"Avg Time (s)\":      m[\"Avg Time (s)\"],\n",
    "            \"Throughput (q/s)\":  m[\"Throughput (q/s)\"],\n",
    "            \"F1\":                m[\"F1\"]\n",
    "        })\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    display(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df, dataset_name):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Create output directory for this dataset\n",
    "    output_dir = f\"plots/{dataset_name.replace(' ', '_')}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Save Individual Metric Plots\n",
    "\n",
    "    def save_metric_plot(x_col, title, filename, data=df, palette=\"Blues_d\"):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=x_col, y='Filter', data=data, palette=palette)\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        path = os.path.join(output_dir, filename)\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "        print(f\"{title} plot saved to: {path}\")\n",
    "\n",
    "    save_metric_plot('Mem (bytes)', 'Memory Usage (bytes)', \"memory_usage.png\", palette=\"Blues_d\")\n",
    "    save_metric_plot('Avg Time (s)', 'Average Lookup Time (s)', \"avg_lookup_time.png\", palette=\"Greens_d\")\n",
    "    save_metric_plot('FP Rate', 'False Positive Rate', \"false_positive_rate.png\", palette=\"Reds_d\")\n",
    "\n",
    "    df_fnr = df.dropna(subset=['FN Rate'])\n",
    "    if not df_fnr.empty:\n",
    "        save_metric_plot('FN Rate', 'False Negative Rate (ML Filters Only)', \"false_negative_rate.png\", data=df_fnr, palette=\"Oranges_d\")\n",
    "\n",
    "    df_f1 = df.dropna(subset=['F1'])\n",
    "    if not df_f1.empty:\n",
    "        save_metric_plot('F1', 'F1 Score (ML Filters Only)', \"f1_score.png\", data=df_f1, palette=\"Purples_d\")\n",
    "\n",
    "    save_metric_plot('Throughput (q/s)', 'Throughput (queries/sec)', \"throughput.png\", palette=\"Greys_d\")\n",
    "\n",
    "    # 2. Combined Plot\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 16))\n",
    "    fig.suptitle(f'{dataset_name} Bloom Filter Evaluation Metrics', fontsize=20)\n",
    "\n",
    "    sns.barplot(x='Mem (bytes)', y='Filter', data=df, ax=axes[0, 0], palette=\"Blues_d\")\n",
    "    axes[0, 0].set_title('Memory Usage (bytes)')\n",
    "\n",
    "    sns.barplot(x='Avg Time (s)', y='Filter', data=df, ax=axes[0, 1], palette=\"Greens_d\")\n",
    "    axes[0, 1].set_title('Average Lookup Time (s)')\n",
    "\n",
    "    sns.barplot(x='FP Rate', y='Filter', data=df, ax=axes[1, 0], palette=\"Reds_d\")\n",
    "    axes[1, 0].set_title('False Positive Rate')\n",
    "\n",
    "    if not df_fnr.empty:\n",
    "        sns.barplot(x='FN Rate', y='Filter', data=df_fnr, ax=axes[1, 1], palette=\"Oranges_d\")\n",
    "        axes[1, 1].set_title('False Negative Rate (ML Filters Only)')\n",
    "\n",
    "    if not df_f1.empty:\n",
    "        sns.barplot(x='F1', y='Filter', data=df_f1, ax=axes[2, 0], palette=\"Purples_d\")\n",
    "        axes[2, 0].set_title('F1 Score (ML Filters Only)')\n",
    "\n",
    "    sns.barplot(x='Throughput (q/s)', y='Filter', data=df, ax=axes[2, 1], palette=\"Greys_d\")\n",
    "    axes[2, 1].set_title('Throughput (queries/sec)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    \n",
    "    combined_path = os.path.join(output_dir, f\"{dataset_name.replace(' ', '_')}_combined_metrics.png\")\n",
    "    plt.savefig(combined_path)\n",
    "    print(f\"Combined plot saved to: {combined_path}\")\n",
    "    plt.show()  # Show the combined plot inline\n",
    "\n",
    "    # 3. Save DataFrame\n",
    "    csv_path = os.path.join(output_dir, f\"{dataset_name.replace(' ', '_')}_results.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"DataFrame saved to: {csv_path}\")\n",
    "\n",
    "    display(df)  # Show the DataFrame inline in Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_email = evaluate_dataset(\n",
    "    \"Email Dataset\",\n",
    "    \"../datasets/emails/spam_email_positives.txt\",\n",
    "    \"../datasets/emails/spam_email_negatives.txt\"\n",
    ")\n",
    "\n",
    "plot_results(df_email, \"Email Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_passwords = evaluate_dataset(\n",
    "    \"Password Dataset\",\n",
    "    \"../datasets/passwords/password_positives.txt\",\n",
    "    \"../datasets/passwords/password_negatives.txt\"\n",
    ")\n",
    "\n",
    "plot_results(df_passwords, \"Password Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ip = evaluate_dataset(\n",
    "    \"IP Address Dataset\",\n",
    "    \"../datasets/ip_addresses/ip_address_positives.txt\",\n",
    "    \"../datasets/ip_addresses/ip_addresses_negatives.txt\"\n",
    ")\n",
    "\n",
    "plot_results(df_ip, \"IP Address Dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phone = evaluate_dataset(\n",
    "    \"Phone Number Dataset\",\n",
    "    \"../datasets/phone_numbers/phone_numbers_positives.txt\",\n",
    "    \"../datasets/phone_numbers/phone_numbers_negatives.txt\"\n",
    ")\n",
    "\n",
    "plot_results(df_phone, \"Phone Number Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_url = evaluate_dataset(\n",
    "    \"URL Dataset\",\n",
    "    \"../datasets/urls/url_positives.txt\",\n",
    "    \"../datasets/urls/url_negatives.txt\"\n",
    ")\n",
    "\n",
    "plot_results(df_url, \"URL Dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
