{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections.abc import Mapping, Container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_getsizeof(o, seen=None):\n",
    "    if seen is None: seen = set()\n",
    "    oid = id(o)\n",
    "    if oid in seen:\n",
    "        return 0\n",
    "    seen.add(oid)\n",
    "\n",
    "    size = sys.getsizeof(o)\n",
    "\n",
    "    if isinstance(o, np.ndarray):\n",
    "        return size + o.nbytes\n",
    "    if isinstance(o, Mapping):\n",
    "        return size + sum(deep_getsizeof(k, seen) + deep_getsizeof(v, seen) for k, v in o.items())\n",
    "    if isinstance(o, Container) and not isinstance(o, (str, bytes, bytearray)):\n",
    "        return size + sum(deep_getsizeof(i, seen) for i in o)\n",
    "    if hasattr(o, '__dict__'):\n",
    "        return size + deep_getsizeof(vars(o), seen)\n",
    "    return size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage(bf):\n",
    "    \"\"\"Estimate memory used by each filter.\"\"\"\n",
    "    size = sys.getsizeof(bf)\n",
    "\n",
    "    # classical bit/count arrays\n",
    "    if hasattr(bf, 'bit_array'):\n",
    "        size += sys.getsizeof(bf.bit_array)\n",
    "    if hasattr(bf, 'count_array'):\n",
    "        size += sys.getsizeof(bf.count_array)\n",
    "\n",
    "    # Sandwich: just sum its two parts\n",
    "    if hasattr(bf, 'small') and hasattr(bf, 'ml'):\n",
    "        return get_memory_usage(bf.ml) + get_memory_usage(bf.small)\n",
    "\n",
    "    # any ML‚Äêbased filter with a .model\n",
    "    if hasattr(bf, 'model'):\n",
    "        mdl = bf.model\n",
    "\n",
    "        # neural networks & logistic regression have coefs_ / intercepts_\n",
    "        if hasattr(mdl, 'coefs_'):\n",
    "            for coef in mdl.coefs_:\n",
    "                size += coef.nbytes\n",
    "            for intercept in mdl.intercepts_:\n",
    "                size += intercept.nbytes\n",
    "\n",
    "        # random forests have many tree estimators\n",
    "        if hasattr(mdl, 'estimators_'):\n",
    "            for tree_est in mdl.estimators_:\n",
    "                tree = tree_est.tree_\n",
    "                # pick the big arrays inside each tree\n",
    "                for arr_name in ('threshold', 'feature', 'children_left',\n",
    "                                 'children_right', 'value'):\n",
    "                    arr = getattr(tree, arr_name, None)\n",
    "                    if isinstance(arr, np.ndarray):\n",
    "                        size += arr.nbytes\n",
    "\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardBloomFilter:\n",
    "    def __init__(self, n, fp_rate):\n",
    "        self.size = self._get_size(n, fp_rate)\n",
    "        self.hash_count = self._get_hash_count(self.size, n)\n",
    "        self.bit_array = [0] * self.size\n",
    "\n",
    "    def _hashes(self, item):\n",
    "        return [hashlib.sha256(f\"{item}{i}\".encode()).hexdigest() for i in range(self.hash_count)]\n",
    "\n",
    "    def _get_size(self, n, p):\n",
    "        m = -(n * math.log(p)) / (math.log(2)**2)\n",
    "        return int(m)\n",
    "\n",
    "    def _get_hash_count(self, m, n):\n",
    "        return int((m / n) * math.log(2))\n",
    "\n",
    "    def add(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            idx = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(), 16) % self.size\n",
    "            self.bit_array[idx] = 1\n",
    "\n",
    "    def query(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            idx = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(), 16) % self.size\n",
    "            if self.bit_array[idx] == 0:\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountingBloomFilter:\n",
    "    def __init__(self, n, fp_rate):\n",
    "        self.size = self._get_size(n, fp_rate)\n",
    "        self.hash_count = self._get_hash_count(self.size, n)\n",
    "        self.count_array = [0] * self.size\n",
    "\n",
    "    def _get_size(self, n, p):\n",
    "        return int(-(n * math.log(p)) / (math.log(2)**2))\n",
    "\n",
    "    def _get_hash_count(self, m, n):\n",
    "        return int((m / n) * math.log(2))\n",
    "\n",
    "    def add(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            idx = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(), 16) % self.size\n",
    "            self.count_array[idx] += 1\n",
    "\n",
    "    def remove(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            idx = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(), 16) % self.size\n",
    "            self.count_array[idx] = max(0, self.count_array[idx] - 1)\n",
    "\n",
    "    def query(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            idx = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(), 16) % self.size\n",
    "            if self.count_array[idx] == 0:\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetworkBloomFilter:\n",
    "    def __init__(self):\n",
    "        self.model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=200)\n",
    "        self.set_members = set()\n",
    "\n",
    "    def _featurize(self, url):\n",
    "        return np.array([ord(c) for c in url[:50]] + [0] * (50 - len(url))).reshape(1, -1)\n",
    "\n",
    "    def train(self, positives, negatives):\n",
    "        X = [self._featurize(x).flatten() for x in positives + negatives]\n",
    "        y = [1]*len(positives) + [0]*len(negatives)\n",
    "        self.model.fit(X, y)\n",
    "        self.set_members = set(positives)\n",
    "\n",
    "    def add(self, item): pass  # Not used\n",
    "\n",
    "    def query(self, item):\n",
    "        x = self._featurize(item)\n",
    "        pred = self.model.predict(x)[0]\n",
    "        if pred == 1:\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestBloomFilter:\n",
    "    def __init__(self,\n",
    "                 max_model_size_bytes=1500000,\n",
    "                 n_estimators=100,\n",
    "                 max_depth=7,\n",
    "                 max_leaf_nodes=None,\n",
    "                 **other_rf_kwargs):\n",
    "        self.size_limit = max_model_size_bytes\n",
    "        self.base_kwargs = dict(n_estimators=n_estimators,\n",
    "                                max_depth=None,            # we‚Äôll set it dynamically\n",
    "                                max_leaf_nodes=max_leaf_nodes,\n",
    "                                **other_rf_kwargs)\n",
    "        self.best_depth = max_depth\n",
    "        self.set_members = set()\n",
    "        self.model = None\n",
    "\n",
    "    def _featurize(self, x):\n",
    "        arr = [ord(c) for c in x[:50]] + [0]*(50 - len(x))\n",
    "        return np.array(arr).reshape(1, -1)\n",
    "\n",
    "    def train(self, positives, negatives):\n",
    "        X = np.vstack([self._featurize(u) for u in positives + negatives])\n",
    "        y = np.array([1]*len(positives) + [0]*len(negatives))\n",
    "        self.set_members = set(positives)\n",
    "\n",
    "        # Try depths from best_depth down to 1\n",
    "        for depth in range(self.best_depth, 0, -1):\n",
    "            kwargs = {**self.base_kwargs, 'max_depth': depth}\n",
    "            candidate = RandomForestClassifier(**kwargs)\n",
    "            candidate.fit(X, y)\n",
    "\n",
    "            self.model = candidate  # temporarily assign to measure total size\n",
    "            size = get_memory_usage(self)\n",
    "            if size <= self.size_limit:\n",
    "                print(f\"‚úÖ depth={depth} full filter is {size:,} bytes ‚â§ {self.size_limit:,}\")\n",
    "                self.best_depth = depth\n",
    "                return  # success, keep self.model\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  depth={depth} full filter is {size:,} bytes > {self.size_limit:,}, trying shallower...\")\n",
    "\n",
    "        # fallback: depth=1 is still too big\n",
    "        print(\"‚ùå could not fit under size limit; using depth=1 model anyway\")\n",
    "        self.model = candidate\n",
    "        self.best_depth = 1\n",
    "\n",
    "\n",
    "    def query(self, item):\n",
    "        if item in self.set_members:\n",
    "            return True\n",
    "        x = self._featurize(item)\n",
    "        return bool(self.model.predict(x)[0])\n",
    "    \n",
    "    def add(self, item):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionBloomFilter:\n",
    "    def __init__(self, **lr_kwargs):\n",
    "        self.model = LogisticRegression(**lr_kwargs)\n",
    "        self.set_members = set()\n",
    "\n",
    "    def _featurize(self, x):\n",
    "        arr = [ord(c) for c in x[:50]] + [0]*(50 - len(x))\n",
    "        return np.array(arr).reshape(1, -1)\n",
    "\n",
    "    def train(self, positives, negatives):\n",
    "        X = np.vstack([self._featurize(u) for u in positives + negatives])\n",
    "        y = np.array([1]*len(positives) + [0]*len(negatives))\n",
    "        self.model.fit(X, y)\n",
    "        self.set_members = set(positives)\n",
    "\n",
    "    def add(self, item):\n",
    "        pass\n",
    "\n",
    "    def query(self, item):\n",
    "        if item in self.set_members:\n",
    "            return True\n",
    "        x = self._featurize(item)\n",
    "        return bool(self.model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SandwichBloomFilter:\n",
    "    def __init__(self, ml_filter, positives, negatives, fp_rate_small=0.20):\n",
    "        \"\"\"\n",
    "        ml_filter: any object with .train(positives, negatives) and .query(item)\n",
    "        \"\"\"\n",
    "        self.ml = ml_filter\n",
    "        self.ml.train(positives, negatives)\n",
    "\n",
    "        # small classical BF to catch anything the ML lets through\n",
    "        self.small = StandardBloomFilter(len(positives), fp_rate_small)\n",
    "        for u in positives:\n",
    "            self.small.add(u)\n",
    "\n",
    "    def add(self, item):\n",
    "        pass\n",
    "\n",
    "    def query(self, item):\n",
    "        if self.ml.predict_proba(item) >= self.threshold:\n",
    "            return True  # ML accepts with high confidence\n",
    "        return self.backup_bf.query(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set memory usage: 2,097,368 bytes (2.00 MB)\n"
     ]
    }
   ],
   "source": [
    "def human_readable_size(num_bytes, decimals=2):\n",
    "    for unit in ('B','KB','MB','GB','TB'):\n",
    "        if num_bytes < 1024.0:\n",
    "            return f\"{num_bytes:.{decimals}f} {unit}\"\n",
    "        num_bytes /= 1024.0\n",
    "    return f\"{num_bytes:.{decimals}f} PB\"\n",
    "\n",
    "# ‚Ä¶ after you build your set ‚Ä¶\n",
    "size = get_memory_usage(my_set)\n",
    "print(f\"Set memory usage: {size:,} bytes ({human_readable_size(size)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(bf, positives, negatives):\n",
    "    \"\"\"Run insertions and then measure FP, FN (for NN), timing, throughput, memory.\"\"\"\n",
    "    # ‚Äî insert positives ‚Äî\n",
    "    for url in positives:\n",
    "        bf.add(url)\n",
    "\n",
    "    # ‚Äî measure false positives ‚Äî\n",
    "    start = time.time()\n",
    "    false_positives = sum(1 for url in negatives if bf.query(url) and url not in positives)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # ‚Äî measure false negatives (only makes sense for NN-based filters) ‚Äî\n",
    "    false_negatives = 0\n",
    "    if isinstance(bf, NeuralNetworkBloomFilter) or isinstance(bf, RandomForestBloomFilter) or isinstance(bf, LogisticRegressionBloomFilter) :\n",
    "        for url in positives:\n",
    "            if not bf.query(url):\n",
    "                false_negatives += 1\n",
    "    fnr = false_negatives / len(positives) if positives else 0.0\n",
    "\n",
    "    # ‚Äî compute metrics ‚Äî\n",
    "    fpr = false_positives / len(negatives)\n",
    "    avg_query_time = elapsed / len(negatives)\n",
    "    throughput = len(negatives) / elapsed if elapsed > 0 else float('inf')\n",
    "    mem_bytes = get_memory_usage(bf)\n",
    "\n",
    "    # ‚Äî print a summary ‚Äî\n",
    "    print(f\"\\n=== {bf.__class__.__name__} ===\")\n",
    "    print(f\"Memory Usage:             {mem_bytes:,} bytes\")\n",
    "    print(f\"False‚ÄêPositive Rate:      {fpr:.4%}\")\n",
    "    if isinstance(bf, NeuralNetworkBloomFilter) or isinstance(bf, RandomForestBloomFilter) or isinstance(bf, LogisticRegressionBloomFilter) :\n",
    "        print(f\"False‚ÄêNegative Rate:      {fnr:.4%}\")\n",
    "    print(f\"Avg Query Time:           {avg_query_time:.6f} s\")\n",
    "    print(f\"Throughput:               {throughput:,.0f} queries/s\")\n",
    "\n",
    "    return {\n",
    "        'fpr': fpr,\n",
    "        'fnr': fnr if isinstance(bf, NeuralNetworkBloomFilter) or isinstance(bf, RandomForestBloomFilter) or isinstance(bf, LogisticRegressionBloomFilter) else None,\n",
    "        'avg_time': avg_query_time,\n",
    "        'throughput': throughput,\n",
    "        'memory_bytes': mem_bytes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê URL Dataset Bloom Filter Evaluation\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[239]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m filters += [(name, bf) \u001b[38;5;28;01mfor\u001b[39;00m name, bf \u001b[38;5;129;01min\u001b[39;00m ml_filters]\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# add a sandwich for each ML filter\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m filters += \u001b[43m[\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSandwich-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSandwichBloomFilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mml_filters\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m]\u001b[49m\n\u001b[32m     26\u001b[39m results_url = []\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, bf \u001b[38;5;129;01min\u001b[39;00m filters:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[239]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     19\u001b[39m filters += [(name, bf) \u001b[38;5;28;01mfor\u001b[39;00m name, bf \u001b[38;5;129;01min\u001b[39;00m ml_filters]\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# add a sandwich for each ML filter\u001b[39;00m\n\u001b[32m     21\u001b[39m filters += [\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSandwich-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[43mSandwichBloomFilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.20\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, bf \u001b[38;5;129;01min\u001b[39;00m ml_filters\n\u001b[32m     24\u001b[39m ]\n\u001b[32m     26\u001b[39m results_url = []\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, bf \u001b[38;5;129;01min\u001b[39;00m filters:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[236]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mSandwichBloomFilter.__init__\u001b[39m\u001b[34m(self, ml_filter, positives, negatives, fp_rate_small)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mml_filter: any object with .train(positives, negatives) and .query(item)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mself\u001b[39m.ml = ml_filter\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegatives\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# small classical BF to catch anything the ML lets through\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.small = StandardBloomFilter(\u001b[38;5;28mlen\u001b[39m(positives), fp_rate_small)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[234]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mRandomForestBloomFilter.train\u001b[39m\u001b[34m(self, positives, negatives)\u001b[39m\n\u001b[32m     28\u001b[39m kwargs = {**\u001b[38;5;28mself\u001b[39m.base_kwargs, \u001b[33m'\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m'\u001b[39m: depth}\n\u001b[32m     29\u001b[39m candidate = RandomForestClassifier(**kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mcandidate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m.model = candidate  \u001b[38;5;66;03m# temporarily assign to measure total size\u001b[39;00m\n\u001b[32m     33\u001b[39m size = get_memory_usage(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:487\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    476\u001b[39m trees = [\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    479\u001b[39m ]\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:189\u001b[39m, in \u001b[36m_parallel_build_trees\u001b[39m\u001b[34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    187\u001b[39m         curr_sample_weight *= compute_sample_weight(\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, y, indices=indices)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m     tree._fit(\n\u001b[32m    198\u001b[39m         X,\n\u001b[32m    199\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m         missing_values_in_feature_mask=missing_values_in_feature_mask,\n\u001b[32m    203\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/tree/_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# üåê URL Dataset\n",
    "print(\"\\U0001f310 URL Dataset Bloom Filter Evaluation\\n\")\n",
    "with open(\"../datasets/urls/url_positives.txt\") as f:\n",
    "    positives = f.read().splitlines()\n",
    "with open(\"../datasets/urls/url_negatives.txt\") as f:\n",
    "    negatives = f.read().splitlines()\n",
    "\n",
    "ml_filters = [\n",
    "    (\"NeuralNet\",      NeuralNetworkBloomFilter()),\n",
    "    (\"RandomForest\",   RandomForestBloomFilter(n_estimators=100)),\n",
    "    (\"LogisticRegr\",   LogisticRegressionBloomFilter(max_iter=200)),\n",
    "]\n",
    "\n",
    "filters = [\n",
    "    (\"Standard\", StandardBloomFilter(len(positives), 0.10)),\n",
    "    (\"Counting\", CountingBloomFilter(len(positives), 0.10)),\n",
    "]\n",
    "# add the plain ML filters\n",
    "filters += [(name, bf) for name, bf in ml_filters]\n",
    "# add a sandwich for each ML filter\n",
    "filters += [\n",
    "    (f\"Sandwich-{name}\", SandwichBloomFilter(bf, positives, negatives, 0.20))\n",
    "    for name, bf in ml_filters\n",
    "]\n",
    "\n",
    "results_url = []\n",
    "for name, bf in filters:\n",
    "    m = evaluate(bf, positives, negatives)\n",
    "    results_url.append({\n",
    "        \"Filter\":            name,\n",
    "        \"Mem (bytes)\":       m[\"memory_bytes\"],\n",
    "        \"FP Rate\":           m[\"fpr\"],\n",
    "        \"FN Rate\":           m[\"fnr\"],\n",
    "        \"Avg Time (s)\":      m[\"avg_time\"],\n",
    "        \"Throughput (q/s)\":  m[\"throughput\"],\n",
    "    })\n",
    "\n",
    "df_url = pd.DataFrame(results_url)\n",
    "display(df_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Password Dataset Bloom Filter Evaluation\n",
      "\n",
      "‚úÖ depth=7 full filter is 148,280 bytes ‚â§ 1,500,000\n",
      "\n",
      "=== StandardBloomFilter ===\n",
      "Memory Usage:             1,265,728 bytes\n",
      "False‚ÄêPositive Rate:      0.0200%\n",
      "Avg Query Time:           0.000002 s\n",
      "Throughput:               435,695 queries/s\n",
      "\n",
      "=== CountingBloomFilter ===\n",
      "Memory Usage:             1,265,728 bytes\n",
      "False‚ÄêPositive Rate:      0.0200%\n",
      "Avg Query Time:           0.000002 s\n",
      "Throughput:               422,387 queries/s\n",
      "\n",
      "=== NeuralNetworkBloomFilter ===\n",
      "Memory Usage:             4,224 bytes\n",
      "False‚ÄêPositive Rate:      0.0600%\n",
      "False‚ÄêNegative Rate:      0.0600%\n",
      "Avg Query Time:           0.000080 s\n",
      "Throughput:               12,456 queries/s\n",
      "\n",
      "=== RandomForestBloomFilter ===\n",
      "Memory Usage:             148,280 bytes\n",
      "False‚ÄêPositive Rate:      0.0000%\n",
      "False‚ÄêNegative Rate:      0.0000%\n",
      "Avg Query Time:           0.003279 s\n",
      "Throughput:               305 queries/s\n",
      "\n",
      "=== LogisticRegressionBloomFilter ===\n",
      "Memory Usage:             56 bytes\n",
      "False‚ÄêPositive Rate:      1.9200%\n",
      "False‚ÄêNegative Rate:      0.0000%\n",
      "Avg Query Time:           0.000071 s\n",
      "Throughput:               14,137 queries/s\n",
      "\n",
      "=== SandwichBloomFilter ===\n",
      "Memory Usage:             272,320 bytes\n",
      "False‚ÄêPositive Rate:      0.0000%\n",
      "Avg Query Time:           0.000082 s\n",
      "Throughput:               12,231 queries/s\n",
      "\n",
      "=== SandwichBloomFilter ===\n",
      "Memory Usage:             416,376 bytes\n",
      "False‚ÄêPositive Rate:      0.0000%\n",
      "Avg Query Time:           0.003401 s\n",
      "Throughput:               294 queries/s\n",
      "\n",
      "=== SandwichBloomFilter ===\n",
      "Memory Usage:             268,152 bytes\n",
      "False‚ÄêPositive Rate:      0.3600%\n",
      "Avg Query Time:           0.000068 s\n",
      "Throughput:               14,767 queries/s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filter</th>\n",
       "      <th>Mem (bytes)</th>\n",
       "      <th>FP Rate</th>\n",
       "      <th>FN Rate</th>\n",
       "      <th>Avg Time (s)</th>\n",
       "      <th>Throughput (q/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Standard</td>\n",
       "      <td>1265728</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>435694.890253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Counting</td>\n",
       "      <td>1265728</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>422387.109768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNet</td>\n",
       "      <td>4224</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>12455.504643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>148280</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>304.968023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegr</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>14136.896175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sandwich-NeuralNet</td>\n",
       "      <td>272320</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>12230.587900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sandwich-RandomForest</td>\n",
       "      <td>416376</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>294.018069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sandwich-LogisticRegr</td>\n",
       "      <td>268152</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>14766.991371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Filter  Mem (bytes)  FP Rate  FN Rate  Avg Time (s)  \\\n",
       "0               Standard      1265728   0.0002      NaN      0.000002   \n",
       "1               Counting      1265728   0.0002      NaN      0.000002   \n",
       "2              NeuralNet         4224   0.0006   0.0006      0.000080   \n",
       "3           RandomForest       148280   0.0000   0.0000      0.003279   \n",
       "4           LogisticRegr           56   0.0192   0.0000      0.000071   \n",
       "5     Sandwich-NeuralNet       272320   0.0000      NaN      0.000082   \n",
       "6  Sandwich-RandomForest       416376   0.0000      NaN      0.003401   \n",
       "7  Sandwich-LogisticRegr       268152   0.0036      NaN      0.000068   \n",
       "\n",
       "   Throughput (q/s)  \n",
       "0     435694.890253  \n",
       "1     422387.109768  \n",
       "2      12455.504643  \n",
       "3        304.968023  \n",
       "4      14136.896175  \n",
       "5      12230.587900  \n",
       "6        294.018069  \n",
       "7      14766.991371  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üîê Password Dataset\n",
    "print(\"\\U0001f510 Password Dataset Bloom Filter Evaluation\\n\")\n",
    "with open(\"../datasets/passwords/password_positives.txt\") as f:\n",
    "    positives = f.read().splitlines()\n",
    "with open(\"../datasets/passwords/password_negatives.txt\") as f:\n",
    "    negatives = f.read().splitlines()\n",
    "\n",
    "filters_pw = [\n",
    "    (\"Standard\", StandardBloomFilter(len(positives), 0.0005)),\n",
    "    (\"Counting\", CountingBloomFilter(len(positives), 0.0005)),\n",
    "] + [(name, bf) for name, bf in ml_filters] + [\n",
    "    (f\"Sandwich-{name}\", SandwichBloomFilter(bf, positives, negatives, 0.20))\n",
    "    for name, bf in ml_filters\n",
    "]\n",
    "\n",
    "results_pw = []\n",
    "for name, bf in filters_pw:\n",
    "    m = evaluate(bf, positives, negatives)\n",
    "    results_pw.append({\n",
    "        \"Filter\":            name,\n",
    "        \"Mem (bytes)\":       m[\"memory_bytes\"],\n",
    "        \"FP Rate\":           m[\"fpr\"],\n",
    "        \"FN Rate\":           m[\"fnr\"],\n",
    "        \"Avg Time (s)\":      m[\"avg_time\"],\n",
    "        \"Throughput (q/s)\":  m[\"throughput\"],\n",
    "    })\n",
    "\n",
    "df_pw = pd.DataFrame(results_pw)\n",
    "display(df_pw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè IP Address Dataset Bloom Filter Evaluation\n",
      "\n",
      "‚úÖ depth=7 full filter is 1,003,544 bytes ‚â§ 1,500,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milesbramwit/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== StandardBloomFilter ===\n",
      "Memory Usage:             3,067,328 bytes\n",
      "False‚ÄêPositive Rate:      0.9625%\n",
      "Avg Query Time:           0.000007 s\n",
      "Throughput:               142,390 queries/s\n",
      "\n",
      "=== CountingBloomFilter ===\n",
      "Memory Usage:             3,067,328 bytes\n",
      "False‚ÄêPositive Rate:      0.9625%\n",
      "Avg Query Time:           0.000007 s\n",
      "Throughput:               142,695 queries/s\n",
      "\n",
      "=== NeuralNetworkBloomFilter ===\n",
      "Memory Usage:             4,224 bytes\n",
      "False‚ÄêPositive Rate:      17.9525%\n",
      "False‚ÄêNegative Rate:      17.7925%\n",
      "Avg Query Time:           0.000170 s\n",
      "Throughput:               5,892 queries/s\n",
      "\n",
      "=== RandomForestBloomFilter ===\n",
      "Memory Usage:             1,003,544 bytes\n",
      "False‚ÄêPositive Rate:      7.4800%\n",
      "False‚ÄêNegative Rate:      0.0000%\n",
      "Avg Query Time:           0.003409 s\n",
      "Throughput:               293 queries/s\n",
      "\n",
      "=== LogisticRegressionBloomFilter ===\n",
      "Memory Usage:             56 bytes\n",
      "False‚ÄêPositive Rate:      16.0550%\n",
      "False‚ÄêNegative Rate:      0.0000%\n",
      "Avg Query Time:           0.000147 s\n",
      "Throughput:               6,805 queries/s\n",
      "\n",
      "=== SandwichBloomFilter ===\n",
      "Memory Usage:             1,076,280 bytes\n",
      "False‚ÄêPositive Rate:      3.6200%\n",
      "Avg Query Time:           0.000102 s\n",
      "Throughput:               9,783 queries/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[215]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m results_ip = []\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, bf \u001b[38;5;129;01min\u001b[39;00m filters_ip:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     m = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegatives\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     results_ip.append({\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFilter\u001b[39m\u001b[33m\"\u001b[39m:            name,\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMem (bytes)\u001b[39m\u001b[33m\"\u001b[39m:       m[\u001b[33m\"\u001b[39m\u001b[33mmemory_bytes\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThroughput (q/s)\u001b[39m\u001b[33m\"\u001b[39m:  m[\u001b[33m\"\u001b[39m\u001b[33mthroughput\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     26\u001b[39m     })\n\u001b[32m     28\u001b[39m df_ip = pd.DataFrame(results_ip)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[212]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(bf, positives, negatives)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ‚Äî measure false positives ‚Äî\u001b[39;00m\n\u001b[32m      8\u001b[39m start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m false_positives = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m negatives \u001b[38;5;28;01mif\u001b[39;00m bf.query(url) \u001b[38;5;129;01mand\u001b[39;00m url \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m positives)\n\u001b[32m     10\u001b[39m elapsed = time.time() - start\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ‚Äî measure false negatives (only makes sense for NN-based filters) ‚Äî\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[212]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ‚Äî measure false positives ‚Äî\u001b[39;00m\n\u001b[32m      8\u001b[39m start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m false_positives = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m negatives \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m url \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m positives)\n\u001b[32m     10\u001b[39m elapsed = time.time() - start\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ‚Äî measure false negatives (only makes sense for NN-based filters) ‚Äî\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[210]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mSandwichBloomFilter.query\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.small.query(item)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[208]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mRandomForestBloomFilter.query\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     50\u001b[39m x = \u001b[38;5;28mself\u001b[39m._featurize(item)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:904\u001b[39m, in \u001b[36mForestClassifier.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    884\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    885\u001b[39m \u001b[33;03m    Predict class for X.\u001b[39;00m\n\u001b[32m    886\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    902\u001b[39m \u001b[33;03m        The predicted classes.\u001b[39;00m\n\u001b[32m    903\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m:\n\u001b[32m    907\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes_.take(np.argmax(proba, axis=\u001b[32m1\u001b[39m), axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:957\u001b[39m, in \u001b[36mForestClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    952\u001b[39m all_proba = [\n\u001b[32m    953\u001b[39m     np.zeros((X.shape[\u001b[32m0\u001b[39m], j), dtype=np.float64)\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np.atleast_1d(\u001b[38;5;28mself\u001b[39m.n_classes_)\n\u001b[32m    955\u001b[39m ]\n\u001b[32m    956\u001b[39m lock = threading.Lock()\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msharedmem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimators_\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m proba \u001b[38;5;129;01min\u001b[39;00m all_proba:\n\u001b[32m    963\u001b[39m     proba /= \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:724\u001b[39m, in \u001b[36m_accumulate_prediction\u001b[39m\u001b[34m(predict, X, out, lock)\u001b[39m\n\u001b[32m    720\u001b[39m         tags.input_tags.allow_nan = get_tags(estimator).input_tags.allow_nan\n\u001b[32m    721\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tags\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[32m    725\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    726\u001b[39m \u001b[33;03m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[32m    727\u001b[39m \n\u001b[32m    728\u001b[39m \u001b[33;03m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[32m    729\u001b[39m \u001b[33;03m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[32m    730\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    731\u001b[39m     prediction = predict(X, check_input=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# üìè IP Address Dataset\n",
    "print(\"\\U0001f4cf IP Address Dataset Bloom Filter Evaluation\\n\")\n",
    "with open(\"../datasets/ip_addresses/ip_address_positives.txt\") as f:\n",
    "    positives = f.read().splitlines()\n",
    "with open(\"../datasets/ip_addresses/ip_addresses_negatives.txt\") as f:\n",
    "    negatives = f.read().splitlines()\n",
    "\n",
    "filters_ip = [\n",
    "    (\"Standard\", StandardBloomFilter(len(positives), 0.01)),\n",
    "    (\"Counting\", CountingBloomFilter(len(positives), 0.01)),\n",
    "] + [(name, bf) for name, bf in ml_filters] + [\n",
    "    (f\"Sandwich-{name}\", SandwichBloomFilter(bf, positives, negatives, 0.20))\n",
    "    for name, bf in ml_filters\n",
    "]\n",
    "\n",
    "results_ip = []\n",
    "for name, bf in filters_ip:\n",
    "    m = evaluate(bf, positives, negatives)\n",
    "    results_ip.append({\n",
    "        \"Filter\":            name,\n",
    "        \"Mem (bytes)\":       m[\"memory_bytes\"],\n",
    "        \"FP Rate\":           m[\"fpr\"],\n",
    "        \"FN Rate\":           m[\"fnr\"],\n",
    "        \"Avg Time (s)\":      m[\"avg_time\"],\n",
    "        \"Throughput (q/s)\":  m[\"throughput\"],\n",
    "    })\n",
    "\n",
    "df_ip = pd.DataFrame(results_ip)\n",
    "display(df_ip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìû Phone Number Dataset Bloom Filter Evaluation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milesbramwit/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== StandardBloomFilter ===\n",
      "Memory Usage:             62,528 bytes\n",
      "False‚ÄêPositive Rate:      1.5000%\n",
      "Avg Query Time:           0.000002 s\n",
      "Throughput:               439,516 queries/s\n",
      "\n",
      "=== CountingBloomFilter ===\n",
      "Memory Usage:             62,528 bytes\n",
      "False‚ÄêPositive Rate:      1.5000%\n",
      "Avg Query Time:           0.000002 s\n",
      "Throughput:               433,654 queries/s\n",
      "\n",
      "=== NeuralNetworkBloomFilter ===\n",
      "Memory Usage:             4,224 bytes\n",
      "False‚ÄêPositive Rate:      37.4000%\n",
      "False‚ÄêNegative Rate:      31.9410%\n",
      "Avg Query Time:           0.000083 s\n",
      "Throughput:               12,099 queries/s\n",
      "\n",
      "=== RandomForestBloomFilter ===\n",
      "Memory Usage:             4,042,712 bytes\n",
      "False‚ÄêPositive Rate:      0.0000%\n",
      "False‚ÄêNegative Rate:      0.0000%\n",
      "Avg Query Time:           0.003559 s\n",
      "Throughput:               281 queries/s\n",
      "\n",
      "=== LogisticRegressionBloomFilter ===\n",
      "Memory Usage:             56 bytes\n",
      "False‚ÄêPositive Rate:      26.4000%\n",
      "False‚ÄêNegative Rate:      0.0000%\n",
      "Avg Query Time:           0.000068 s\n",
      "Throughput:               14,666 queries/s\n",
      "\n",
      "=== SandwichBloomFilter ===\n",
      "Memory Usage:             26,144 bytes\n",
      "False‚ÄêPositive Rate:      7.2000%\n",
      "Avg Query Time:           0.000081 s\n",
      "Throughput:               12,285 queries/s\n",
      "\n",
      "=== SandwichBloomFilter ===\n",
      "Memory Usage:             4,064,632 bytes\n",
      "False‚ÄêPositive Rate:      0.0000%\n",
      "Avg Query Time:           0.003690 s\n",
      "Throughput:               271 queries/s\n",
      "\n",
      "=== SandwichBloomFilter ===\n",
      "Memory Usage:             21,976 bytes\n",
      "False‚ÄêPositive Rate:      5.7000%\n",
      "Avg Query Time:           0.000068 s\n",
      "Throughput:               14,744 queries/s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filter</th>\n",
       "      <th>Mem (bytes)</th>\n",
       "      <th>FP Rate</th>\n",
       "      <th>FN Rate</th>\n",
       "      <th>Avg Time (s)</th>\n",
       "      <th>Throughput (q/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Standard</td>\n",
       "      <td>62528</td>\n",
       "      <td>0.015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>439516.294666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Counting</td>\n",
       "      <td>62528</td>\n",
       "      <td>0.015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>433654.259719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNet</td>\n",
       "      <td>4224</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.31941</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>12098.593215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>4042712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>280.987910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegr</td>\n",
       "      <td>56</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>14666.167806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sandwich-NeuralNet</td>\n",
       "      <td>26144</td>\n",
       "      <td>0.072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>12285.132669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sandwich-RandomForest</td>\n",
       "      <td>4064632</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>271.011899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sandwich-LogisticRegr</td>\n",
       "      <td>21976</td>\n",
       "      <td>0.057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>14744.016170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Filter  Mem (bytes)  FP Rate  FN Rate  Avg Time (s)  \\\n",
       "0               Standard        62528    0.015      NaN      0.000002   \n",
       "1               Counting        62528    0.015      NaN      0.000002   \n",
       "2              NeuralNet         4224    0.374  0.31941      0.000083   \n",
       "3           RandomForest      4042712    0.000  0.00000      0.003559   \n",
       "4           LogisticRegr           56    0.264  0.00000      0.000068   \n",
       "5     Sandwich-NeuralNet        26144    0.072      NaN      0.000081   \n",
       "6  Sandwich-RandomForest      4064632    0.000      NaN      0.003690   \n",
       "7  Sandwich-LogisticRegr        21976    0.057      NaN      0.000068   \n",
       "\n",
       "   Throughput (q/s)  \n",
       "0     439516.294666  \n",
       "1     433654.259719  \n",
       "2      12098.593215  \n",
       "3        280.987910  \n",
       "4      14666.167806  \n",
       "5      12285.132669  \n",
       "6        271.011899  \n",
       "7      14744.016170  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìû Phone Number Dataset\n",
    "print(\"\\U0001f4de Phone Number Dataset Bloom Filter Evaluation\\n\")\n",
    "with open(\"../datasets/phone_numbers/phone_numbers_positives.txt\") as f:\n",
    "    positives = f.read().splitlines()\n",
    "with open(\"../datasets/phone_numbers/phone_numbers_negatives.txt\") as f:\n",
    "    negatives = f.read().splitlines()\n",
    "\n",
    "filters_phone = [\n",
    "    (\"Standard\", StandardBloomFilter(len(positives), 0.01)),\n",
    "    (\"Counting\", CountingBloomFilter(len(positives), 0.01)),\n",
    "] + [(name, bf) for name, bf in ml_filters] + [\n",
    "    (f\"Sandwich-{name}\", SandwichBloomFilter(bf, positives, negatives, 0.20))\n",
    "    for name, bf in ml_filters\n",
    "]\n",
    "\n",
    "results_phone = []\n",
    "for name, bf in filters_phone:\n",
    "    m = evaluate(bf, positives, negatives)\n",
    "    results_phone.append({\n",
    "        \"Filter\":            name,\n",
    "        \"Mem (bytes)\":       m[\"memory_bytes\"],\n",
    "        \"FP Rate\":           m[\"fpr\"],\n",
    "        \"FN Rate\":           m[\"fnr\"],\n",
    "        \"Avg Time (s)\":      m[\"avg_time\"],\n",
    "        \"Throughput (q/s)\":  m[\"throughput\"],\n",
    "    })\n",
    "\n",
    "df_phone = pd.DataFrame(results_phone)\n",
    "display(df_phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß Email Dataset Bloom Filter Evaluation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milesbramwit/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/milesbramwit/cs6386/finalProject/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ depth=7 full filter is 261,176 bytes ‚â§ 1,500,000\n",
      "\n",
      "=== StandardBloomFilter ===\n",
      "Memory Usage:             52,712 bytes\n",
      "False‚ÄêPositive Rate:      1.6012%\n",
      "Avg Query Time:           0.000002 s\n",
      "Throughput:               428,283 queries/s\n",
      "\n",
      "=== CountingBloomFilter ===\n",
      "Memory Usage:             52,712 bytes\n",
      "False‚ÄêPositive Rate:      1.6012%\n",
      "Avg Query Time:           0.000003 s\n",
      "Throughput:               392,359 queries/s\n",
      "\n",
      "=== NeuralNetworkBloomFilter ===\n",
      "Memory Usage:             4,224 bytes\n",
      "False‚ÄêPositive Rate:      10.1892%\n",
      "False‚ÄêNegative Rate:      11.3703%\n",
      "Avg Query Time:           0.000088 s\n",
      "Throughput:               11,335 queries/s\n",
      "\n",
      "=== RandomForestBloomFilter ===\n",
      "Memory Usage:             261,176 bytes\n",
      "False‚ÄêPositive Rate:      0.0000%\n",
      "False‚ÄêNegative Rate:      0.0000%\n",
      "Avg Query Time:           0.003394 s\n",
      "Throughput:               295 queries/s\n",
      "\n",
      "=== LogisticRegressionBloomFilter ===\n",
      "Memory Usage:             56 bytes\n",
      "False‚ÄêPositive Rate:      5.8224%\n",
      "False‚ÄêNegative Rate:      0.0000%\n",
      "Avg Query Time:           0.000067 s\n",
      "Throughput:               14,854 queries/s\n",
      "\n",
      "=== SandwichBloomFilter ===\n",
      "Memory Usage:             22,712 bytes\n",
      "False‚ÄêPositive Rate:      28.8210%\n",
      "Avg Query Time:           0.000085 s\n",
      "Throughput:               11,792 queries/s\n",
      "\n",
      "=== SandwichBloomFilter ===\n",
      "Memory Usage:             279,664 bytes\n",
      "False‚ÄêPositive Rate:      20.2329%\n",
      "Avg Query Time:           0.003415 s\n",
      "Throughput:               293 queries/s\n",
      "\n",
      "=== SandwichBloomFilter ===\n",
      "Memory Usage:             18,544 bytes\n",
      "False‚ÄêPositive Rate:      24.5997%\n",
      "Avg Query Time:           0.000071 s\n",
      "Throughput:               14,076 queries/s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filter</th>\n",
       "      <th>Mem (bytes)</th>\n",
       "      <th>FP Rate</th>\n",
       "      <th>FN Rate</th>\n",
       "      <th>Avg Time (s)</th>\n",
       "      <th>Throughput (q/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Standard</td>\n",
       "      <td>52712</td>\n",
       "      <td>0.016012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>428282.825208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Counting</td>\n",
       "      <td>52712</td>\n",
       "      <td>0.016012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>392359.320261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNet</td>\n",
       "      <td>4224</td>\n",
       "      <td>0.101892</td>\n",
       "      <td>0.113703</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>11334.931133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>261176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>294.658065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegr</td>\n",
       "      <td>56</td>\n",
       "      <td>0.058224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>14854.326657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sandwich-NeuralNet</td>\n",
       "      <td>22712</td>\n",
       "      <td>0.288210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>11791.540040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sandwich-RandomForest</td>\n",
       "      <td>279664</td>\n",
       "      <td>0.202329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>292.789456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sandwich-LogisticRegr</td>\n",
       "      <td>18544</td>\n",
       "      <td>0.245997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>14075.533168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Filter  Mem (bytes)   FP Rate   FN Rate  Avg Time (s)  \\\n",
       "0               Standard        52712  0.016012       NaN      0.000002   \n",
       "1               Counting        52712  0.016012       NaN      0.000003   \n",
       "2              NeuralNet         4224  0.101892  0.113703      0.000088   \n",
       "3           RandomForest       261176  0.000000  0.000000      0.003394   \n",
       "4           LogisticRegr           56  0.058224  0.000000      0.000067   \n",
       "5     Sandwich-NeuralNet        22712  0.288210       NaN      0.000085   \n",
       "6  Sandwich-RandomForest       279664  0.202329       NaN      0.003415   \n",
       "7  Sandwich-LogisticRegr        18544  0.245997       NaN      0.000071   \n",
       "\n",
       "   Throughput (q/s)  \n",
       "0     428282.825208  \n",
       "1     392359.320261  \n",
       "2      11334.931133  \n",
       "3        294.658065  \n",
       "4      14854.326657  \n",
       "5      11791.540040  \n",
       "6        292.789456  \n",
       "7      14075.533168  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìß Email Dataset\n",
    "print(\"\\U0001f4e7 Email Dataset Bloom Filter Evaluation\\n\")\n",
    "with open(\"../datasets/emails/spam_email_positives.txt\") as f:\n",
    "    positives = f.read().splitlines()\n",
    "with open(\"../datasets/emails/spam_email_negatives.txt\") as f:\n",
    "    negatives = f.read().splitlines()\n",
    "\n",
    "filters_email = [\n",
    "    (\"Standard\", StandardBloomFilter(len(positives), 0.01)),\n",
    "    (\"Counting\", CountingBloomFilter(len(positives), 0.01)),\n",
    "] + [(name, bf) for name, bf in ml_filters] + [\n",
    "    (f\"Sandwich-{name}\", SandwichBloomFilter(bf, positives, negatives, 0.20))\n",
    "    for name, bf in ml_filters\n",
    "]\n",
    "\n",
    "results_email = []\n",
    "for name, bf in filters_email:\n",
    "    m = evaluate(bf, positives, negatives)\n",
    "    results_email.append({\n",
    "        \"Filter\":            name,\n",
    "        \"Mem (bytes)\":       m[\"memory_bytes\"],\n",
    "        \"FP Rate\":           m[\"fpr\"],\n",
    "        \"FN Rate\":           m[\"fnr\"],\n",
    "        \"Avg Time (s)\":      m[\"avg_time\"],\n",
    "        \"Throughput (q/s)\":  m[\"throughput\"],\n",
    "    })\n",
    "\n",
    "df_email = pd.DataFrame(results_email)\n",
    "display(df_email)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
